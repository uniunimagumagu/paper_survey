## Introduction
優先度付きで経験再生させたほうがいいよね。
- 更新のあと入力データを捨てるのはもったいない
- 特定の遷移を優先再生させたい：TD誤差が大きいとこ
TD誤差：これが大きいとうまく学習できていない部分であるのでそこを優先させたい。これは過剰に反応する特定の場面を作り出してしまうがランダム性をを取り入れることで緩和する。これにより多様性は維持できたが偏りに関しては重要度サンプリングを行うことで緩和する。
## Background
TDエラーで優先度をつける。
## Prioritized Experience Replay
早くなったことを強調.
# 具体的なアルゴリズム
各遷移の重要性を測定し基準にしている。各遷移の重要性はTD誤差の大きさ。具体的には、次ステップのブートストラップ推定値から離れている値。
これがSARSA、Q学習のようにすでにTD誤差を計算しているようなアルゴリズムに強い。
- 具体的な実装：
・TD誤差を計算、記録(replay memory)。
・優先順位を設定、新しい遷移でまだTD誤差が計算されていない場合は最大値を設定。（必ず経験を参照することで偏りをなくすため）
・遷移を選択、TD誤差が最大のもの。
・更新。